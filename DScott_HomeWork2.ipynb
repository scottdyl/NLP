{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b76fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download(\"book\")\n",
    "from nltk.book import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb8b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second\n",
    "second=open('C:/Users/drsco/Documents/GitHub/NLP/second_reader.txt','r',encoding=\"ISO-8859-1\").read()\n",
    "tokens = nltk.word_tokenize(second)\n",
    "second_t = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8f9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth\n",
    "fourth=open('C:/Users/drsco/Documents/GitHub/NLP/fourth_reader.txt','r',encoding=\"utf8\").read()\n",
    "tokens = nltk.word_tokenize(fourth)\n",
    "fourth_t = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a48f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sixth\n",
    "sixth=open('C:/Users/drsco/Documents/GitHub/NLP/sixth_reader.txt','r',encoding=\"utf8\").read()\n",
    "tokens = nltk.word_tokenize(sixth)\n",
    "sixth_t = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e3edd",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3cde4",
   "metadata": {},
   "source": [
    "1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1293b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size_normal(*text):\n",
    "    #dedicate memeory\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_noramlized = np.array([])\n",
    "    \n",
    "    #size\n",
    "    for t in text:\n",
    "        vocab_size = np.append(vocab_size,len(set(t)))\n",
    "    \n",
    "    #normalizing\n",
    "    for vs in vocab_size:\n",
    "        vocab_size_noramlized = np.append(vocab_size_noramlized,\n",
    "        round((vs-vocab_size.min())/(vocab_size.max() - vocab_size.min()),3))\n",
    "    \n",
    "    return(vocab_size,vocab_size_noramlized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8328b6",
   "metadata": {},
   "source": [
    "I defined a funtion for normalization. The formula takes a list of text. It then takes the difference between text(i) and the min of all the text and divides that by the max text vocab size minus the min text vocab size. \n",
    "This leaves us with the normalized text size on  a scale of 1 to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78a714",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00fb8106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4024.0 14290.0 17259.0\n",
      "0.0 0.776 1.0\n"
     ]
    }
   ],
   "source": [
    "vocab_size = vocab_size_normal(second_t,fourth_t,sixth_t)\n",
    "print(*vocab_size[0])\n",
    "print(*vocab_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa6b5a",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4360e",
   "metadata": {},
   "source": [
    "2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e087737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of charactors in a long word? 7\n"
     ]
    }
   ],
   "source": [
    "#I know the book uses 15\n",
    "#we will set the default \"long word\" length to 7\n",
    "def long_vocab_size_normal(*text, long_word_length = int(input(\"number of charactors in a long word? \"))):\n",
    "    #dedicate memeory\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_noramlized = np.array([])\n",
    "    \n",
    "    #size\n",
    "    for t in text:\n",
    "        set_length = set(t)\n",
    "        long_words = [w for w in set_length if len(w) > long_word_length]\n",
    "        vocab_size = np.append(vocab_size,len(set(long_words)))\n",
    "    \n",
    "    #normalizing\n",
    "    for vs in vocab_size:\n",
    "        vocab_size_noramlized = np.append(vocab_size_noramlized,\n",
    "        round((vs-vocab_size.min())/(vocab_size.max() - vocab_size.min()),3))\n",
    "    \n",
    "    return(vocab_size,vocab_size_noramlized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f533b5",
   "metadata": {},
   "source": [
    "After reading 3.2 from the book, we have determined that longer words have an affect on our vocab size outcomes. For this define function we account for the longer words by using the code from the book. We first take the input of what the user consider a long word, in this case we used 7, and then we check the strings for words over that limit. From there we check the length of each word in the string and pip that into info into our original formula calculating vocab size on a scale of 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8cbb5",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b71a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.744 1.0\n"
     ]
    }
   ],
   "source": [
    "long_vocab = long_vocab_size_normal(second_t,fourth_t,sixth_t)\n",
    "print(*long_vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40d3e6",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea69792",
   "metadata": {},
   "source": [
    "3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f5ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(*text):\n",
    "    lex_div = np.array([])\n",
    "    for t in text:\n",
    "        val = len(set(t)) / len(t)\n",
    "        lex_div = np.append(val)\n",
    "    return(lex_div)\n",
    "# dedicating memeory\n",
    "def text_diff_score(*text, long_word_length = 7):\n",
    "    lex_div = np.array([])\n",
    "    l_vocab_size = np.array([])\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_noramlized = np.array([])\n",
    "    df_normal = np.array([])\n",
    "    longword_normal = np.array([])\n",
    "    \n",
    "    for t in text:\n",
    "        set_length = set(t)\n",
    "        val = len(set(t)) / len(t)\n",
    "        lex_div = np.append(lex_div,val)\n",
    "        long_words = [w for w in set_length if len(w) > long_word_length]\n",
    "        l_vocab_size = np.append(l_vocab_size,len(set(long_words)))\n",
    "        vocab_size = np.append(vocab_size,len(set(t)))\n",
    "        \n",
    "    #normalizing normal\n",
    "    for vs in vocab_size:\n",
    "        vocab_size_noramlized = np.append(vocab_size_noramlized,\n",
    "        (vs-vocab_size.min())/(vocab_size.max() - vocab_size.min()))\n",
    "    \n",
    "    #normalizing long word\n",
    "    for vs in l_vocab_size:\n",
    "        longword_normal = np.append(longword_normal,(vs - l_vocab_size.min()) /\n",
    "                                                    (l_vocab_size.max() - l_vocab_size.min()))\n",
    "    df = lex_div + l_vocab_size + vocab_size_noramlized\n",
    "    df1 = lex_div + longword_normal + vocab_size_noramlized\n",
    "    \n",
    "    ### Normalizing using formula \n",
    "    for df_score in df:\n",
    "        df_normal = np.append(df_normal,(df - df.min())/(df.max() - df.min()))\n",
    "    return(df,df1,df_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d23c3",
   "metadata": {},
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46ac1a",
   "metadata": {},
   "source": [
    "#### Difficulity score with long word vocab normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4523add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1589006476070131 1.6325948437736248 2.1008726044290666\n"
     ]
    }
   ],
   "source": [
    "answer3 = text_diff_score(second_t,fourth_t,sixth_t)\n",
    "print(*answer3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3b137",
   "metadata": {},
   "source": [
    "#### Difficulity score no long word vocab normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee01f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876.158900647607 5356.888532395675 6898.100872604429\n"
     ]
    }
   ],
   "source": [
    "answer3 = text_diff_score(second_t,fourth_t,sixth_t)\n",
    "print(*answer3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb57ce",
   "metadata": {},
   "source": [
    "#### Difficulity score normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482d076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.7440672216062654 1.0 0.0 0.7440672216062654 1.0 0.0 0.7440672216062654 1.0\n"
     ]
    }
   ],
   "source": [
    "answer3 = text_diff_score(second_t,fourth_t,sixth_t)\n",
    "print(*answer3[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c6d425",
   "metadata": {},
   "source": [
    "I feel like this is a good, better, best situation. Situation. Though scoring things by long words may help understand the data, especially regarding children, it is not the end all be all. Lexical diversity can help identify those students who have a large vocabulary but may not use longer words. Overall, I feel like by scaling the data we were able to reduce the bias in each of the categories of long words or lexical diversity. This becomes important when looking at diverse data that could be subject to bias in any of these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759833d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
